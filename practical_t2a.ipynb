{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Thursday: Text-to-Audio Generation\n",
    "\n",
    "This notebook aims to give a brief view of modern model for text to audio generation. Basically, it contains two main models:\n",
    "- [EnCodec](https://arxiv.org/abs/2210.13438), a ResidualVQ based audio codec model to compress the raw wavform into discrete tokens \n",
    "- [AudioGen](https://arxiv.org/abs/2209.15352), a Transformer based audio-language model\n",
    "\n",
    "In this lab session, we will train a EnCodec modelon a toy dataset, from data preparation to model configuration. Then, we will compare it with the EnCodec model from MetaAI which is fully pretrained on a large dataset. Finally we will see how the pretrained audio codec model can be used for text-to-audio generation, by using the AudioGen model pretrained by MetaAI also.\n",
    "\n",
    "This notebook is inspired from the [AudioCraft](https://github.com/facebookresearch/audiocraft) project, you could check more details in their repo. \n",
    "\n",
    "For any other questions, pleas contact xiaoyu[dot]bie[at]telecom-paris[dot]fr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "python -m pip install -r 'requirements.txt'\n",
    "python -m pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import IPython\n",
    "import julius\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load config\n",
    "cfg_filepath = 'encodec16k.yaml'\n",
    "cfg = OmegaConf.load(cfg_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnCodec\n",
    "\n",
    "### Pipeline figure, from the [EnCodec](https://arxiv.org/abs/2210.13438)\n",
    "![](assets/arc_encodec.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataloader\n",
    "\n",
    "To prepare a dataloader, we first create a csv file which contains the meta information (filepath, sample rate, etc) of all the training data\n",
    "\n",
    "The data directory is defined in `cfg.toy_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiodata import DatasetAudioTrain\n",
    "\n",
    "audio_dir = Path(cfg.toy_data)\n",
    "audio_manifest = 'mani_audio.csv'\n",
    "ext = 'wav'\n",
    "\n",
    "## Prepare data metadata\n",
    "audio_len = 0\n",
    "with open(audio_manifest, 'w') as f:\n",
    "    f.write('id,filepath,sr,length\\n') # libri-light too large, no silence trim\n",
    "    for audio_filepath in tqdm(sorted(list(audio_dir.glob(f'**/*.{ext}'))), desc=f'prepare..'):\n",
    "        audio_id = audio_filepath.stem\n",
    "        x, sr = torchaudio.load(audio_filepath)\n",
    "        length = x.shape[-1]\n",
    "        utt_len = length / sr\n",
    "        audio_len += utt_len\n",
    "        line = '{},{},{},{}\\n'.format(audio_id, audio_filepath, sr, length)\n",
    "        f.write(line)\n",
    "    print('Total audio len: {:.2f} mins'.format(audio_len/60))\n",
    "\n",
    "\n",
    "## get dataloader\n",
    "dataset = DatasetAudioTrain(csv_file=audio_manifest,\n",
    "                            sample_rate=cfg.sample_rate,\n",
    "                            n_examples=cfg.dataset.n_examples,\n",
    "                            chunk_size=cfg.dataset.segment_duration,\n",
    "                            trim_silence=cfg.dataset.trim_silence,\n",
    "                            normalize=cfg.dataset.normalize,\n",
    "                            lufs_norm_db=cfg.dataset.lufs_norm_db,\n",
    "                            lufs_var=cfg.dataset.lufs_var)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, \n",
    "                        batch_size=cfg.dataset.batch_size, num_workers=cfg.dataset.num_workers,\n",
    "                        shuffle=cfg.dataset.shuffle, drop_last=cfg.dataset.drop_last)\n",
    "print('Batch size: {}, {} iterations per epoch'.format(cfg.dataset.batch_size, len(dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model and loss\n",
    "\n",
    "The training of EnCodec is similar to [VQGAN](https://arxiv.org/abs/2012.09841), which contains two main parts:\n",
    "- VQ-VAE, for vector quantization and data reconstruction\n",
    "- Discriminator, for adversarial training\n",
    "\n",
    "Futhermore, EnCodec introduces a loss balancer to stabilize training. Defining the gradients $g_i = \\frac{\\partial l_i}{\\partial \\hat{x}}$, and $\\langle || g_i ||_2 \\rangle_{\\beta}$ the exponential moving average of $g_i$. Given a set of weights $\\lambda_i$, and a reference norm $R$ it has:\n",
    "\n",
    "$$\n",
    "\\hat{g}_i = R \\frac{\\lambda_i}{\\sum \\lambda_i} \\cdot \\frac{g_i}{\\langle || g_i ||_2 \\rangle_{\\beta}}\n",
    "$$\n",
    "\n",
    "In practice, $R = 1$ and $\\beta = 0.999$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.solvers.builders import (\n",
    "    get_optimizer,\n",
    "    get_audio_datasets,\n",
    "    get_adversarial_losses,\n",
    "    get_loss,\n",
    "    get_balancer\n",
    ")\n",
    "from audiocraft.models.builders import get_compression_model\n",
    "\n",
    "\n",
    "## get model and optimizer\n",
    "model = get_compression_model(cfg.model)\n",
    "optimizer = get_optimizer(model.parameters(), cfg.optim)\n",
    "print('Use {} optimizer, learning rate: {}'.format(cfg.optim.optimizer, cfg.optim.lr))\n",
    "\n",
    "# get loss function\n",
    "adv_losses = get_adversarial_losses(cfg)\n",
    "aux_losses = torch.nn.ModuleDict()\n",
    "loss_weights = dict()\n",
    "for loss_name, weight in cfg.losses.items():\n",
    "    if loss_name in ['adv', 'feat']:\n",
    "        for adv_name, _ in adv_losses.items():\n",
    "            loss_weights[f'{loss_name}_{adv_name}'] = weight\n",
    "    elif weight > 0:\n",
    "        aux_losses[loss_name] = get_loss(loss_name, cfg)\n",
    "        loss_weights[loss_name] = weight\n",
    "balancer = get_balancer(loss_weights, cfg.balancer)\n",
    "print(\"Total # of params: {:.2f} M\".format(sum(p.numel() for p in model.parameters())/1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Due to time constraints, we only performed preliminary training on a toy dataset containing only 10 audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "ckpt_path = 'last_ckpt.pth'\n",
    "total_epoch = cfg.optim.epochs\n",
    "model = model.to(cfg.device)\n",
    "model.train()\n",
    "print('Training epoch: {}'.format(total_epoch))\n",
    "for epo in range(total_epoch):\n",
    "    for audio_data in tqdm(dataloader, total=len(dataloader)):\n",
    "        # prepare data\n",
    "        x = audio_data.to(cfg.device)\n",
    "        y = x.clone()\n",
    "        metrics = {}\n",
    "\n",
    "        # forward\n",
    "        qres = model(x)\n",
    "        y_pred = qres.x\n",
    "\n",
    "        # discrimilator loss\n",
    "        d_losses: dict = {}\n",
    "        for adv_name, adversary in adv_losses.items():\n",
    "            disc_loss = adversary.train_adv(y_pred, y)\n",
    "            d_losses[f'd_{adv_name}'] = disc_loss\n",
    "        metrics['d_loss'] = torch.sum(torch.stack(list(d_losses.values())))\n",
    "        \n",
    "        balanced_losses: dict = {}\n",
    "        other_losses: dict = {}\n",
    "\n",
    "        # penalty from quantization\n",
    "        if qres.penalty is not None and qres.penalty.requires_grad:\n",
    "            other_losses['penalty'] = qres.penalty  # penalty term from the quantizer\n",
    "\n",
    "        # adversarial losses\n",
    "        for adv_name, adversary in adv_losses.items():\n",
    "            adv_loss, feat_loss = adversary(y_pred, y)\n",
    "            balanced_losses[f'adv_{adv_name}'] = adv_loss\n",
    "            balanced_losses[f'feat_{adv_name}'] = feat_loss\n",
    "\n",
    "        # auxiliary losses\n",
    "        for loss_name, criterion in aux_losses.items():\n",
    "            loss = criterion(y_pred, y)\n",
    "            balanced_losses[loss_name] = loss\n",
    "\n",
    "        # backprop losses that are not handled by balancer\n",
    "        other_loss = torch.tensor(0., device=cfg.device)\n",
    "        if 'penalty' in other_losses:\n",
    "            other_loss += other_losses['penalty']\n",
    "        if other_loss.requires_grad:\n",
    "            other_loss.backward(retain_graph=True)\n",
    "        \n",
    "        # balancer losses backward\n",
    "        metrics['g_loss'] = balancer.backward(balanced_losses, y_pred)\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # save model every epoch\n",
    "    print('====> Epoch: {}, d_loss: {:.3f}, g_loss: {:.3f}'.format(epo, metrics['d_loss'], metrics['g_loss']))\n",
    "    torch.save({\n",
    "            'epoch': epo,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': metrics,\n",
    "            }, ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Compression\n",
    "\n",
    "Once the training is finished, we can use the model to reconstruct the input audio via:\n",
    "- **Encoder**: $z = Enc(x)$\n",
    "- **Quantizer**: $z_q = Quant(z)$\n",
    "- **Decoder**: $y = Dec(z_q)$\n",
    "\n",
    "We will see the performance of the model trained on toy dataset, compared with the EnCodec model fully trained from [facebook/encodec_32khz](https://huggingface.co/facebook/encodec_32khz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an example audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load an audio\n",
    "audio_filepath = 'example.wav'\n",
    "x, fs = torchaudio.load(audio_filepath)\n",
    "print('Audio length: {:.1f}s'.format(x.shape[-1]/fs))\n",
    "\n",
    "## Display the audio\n",
    "print('Original Audio:')\n",
    "IPython.display.Audio(audio_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction through the model trained on toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_filepath = 'example_recon.wav'\n",
    "\n",
    "## Load model\n",
    "model = get_compression_model(cfg.model)\n",
    "checkpoint = torch.load('last_ckpt.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.cpu().eval()\n",
    "\n",
    "## Preprocess\n",
    "x_in = julius.resample_frac(x, old_sr=fs, new_sr=16000)\n",
    "\n",
    "## Reconstruction\n",
    "codes, scale = model.encode(x_in[None,])\n",
    "y = model.decode(codes, scale)[0].detach()\n",
    "\n",
    "## Write the audio\n",
    "torchaudio.save(recon_filepath, y, sample_rate=16000)\n",
    "\n",
    "## Display the audio\n",
    "print('Reconstructed Audio from the model trained on toy dataset:')\n",
    "IPython.display.Audio(recon_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction through the model from facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import CompressionModel\n",
    "recon_filepath = 'example_recon_fb.wav'\n",
    "\n",
    "## Load model\n",
    "model_fb = CompressionModel.get_pretrained('facebook/encodec_32khz')\n",
    "\n",
    "## Preprocess, they don't provide 16k model, so we use 32k instead\n",
    "x_in = julius.resample_frac(x, old_sr=fs, new_sr=32000)\n",
    "codes, scale = model_fb.encode(x_in[None,])\n",
    "y_fb = model_fb.decode(codes, scale)[0].detach()\n",
    "y_fb = julius.resample_frac(y_fb, old_sr=32000, new_sr=16000)\n",
    "torchaudio.save(recon_filepath, y_fb, sample_rate=16000)\n",
    "\n",
    "## Display the audio\n",
    "print('Reconstructed Audio from the model trained on toy dataset:')\n",
    "IPython.display.Audio(recon_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Audio Generation\n",
    "\n",
    "Once we have a well-trained audio codec model, we can incoorperate it with:\n",
    "- **Text Encoder**, turn the text information into feature reprensentations\n",
    "- **Audio Language Model**, a Transformer-based LLM that predicts the audio latent codes based on the text condidtion\n",
    "\n",
    "\n",
    "### Pipeline figure, from the [AudioGen](https://arxiv.org/abs/2209.15352)\n",
    "![](assets/arc_audiogen.png)\n",
    "\n",
    "Due to the time and resource limitation, we directly use the pretrained AudioGen model from [facebook/audiogen-medium](https://huggingface.co/facebook/audiogen-medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import AudioGen\n",
    "from audiocraft.data.audio import audio_write\n",
    "\n",
    "\n",
    "## you can use any customized text prompt\n",
    "description = 'dog barking'\n",
    "# description = 'sirene of an emergency vehicle'\n",
    "# description = 'footsteps in a corridor'\n",
    "\n",
    "model = AudioGen.get_pretrained('facebook/audiogen-medium')\n",
    "model.set_generation_params(duration=5)\n",
    "wav = model.generate([description], progress=True)[0]\n",
    "audio_write('sample', wav.cpu(), model.sample_rate, strategy=\"loudness\", loudness_compressor=True) # -14 db LUFS\n",
    "\n",
    "print('Description {}'.format(description))\n",
    "IPython.display.Audio('sample.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiocraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
