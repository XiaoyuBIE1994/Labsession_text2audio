{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Thursday: Text-to-Audio Generation\n",
    "\n",
    "This notebook aims to give a brief view of modern model for text to audio generation. Basically, it contains two main models:\n",
    "- [EnCodec](https://arxiv.org/abs/2210.13438), a ResidualVQ based audio codec model to compress the raw wavform into discrete tokens \n",
    "- [AudioGen](https://arxiv.org/abs/2209.15352), a Transformer based audio-language model\n",
    "\n",
    "In this lab session, we will train a EnCodec modelon a toy dataset, from data preparation to model configuration. Then, we will compare it with the EnCodec model from MetaAI which is fully pretrained on a large dataset. Finally we will see how the pretrained audio codec model can be used for text-to-audio generation, by using the AudioGen model pretrained by MetaAI also.\n",
    "\n",
    "This notebook is inspired from the [AudioCraft](https://github.com/facebookresearch/audiocraft) project, you could check more details in their repo. \n",
    "\n",
    "For any other questions, pleas contact xiaoyu[dot]bie[at]telecom-paris[dot]fr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install \n",
    "python -m pip install -r 'requirements.txt'\n",
    "python -m pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import IPython\n",
    "import julius\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load config\n",
    "cfg_filepath = 'encodec16k.yaml'\n",
    "cfg = OmegaConf.load(cfg_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiodata import DatasetAudioTrain\n",
    "\n",
    "audio_dir = Path(cfg.toy_data)\n",
    "audio_manifest = 'mani_audio.csv'\n",
    "ext = 'wav'\n",
    "\n",
    "## Prepare data metadata\n",
    "audio_len = 0\n",
    "with open(audio_manifest, 'w') as f:\n",
    "    f.write('id,filepath,sr,length\\n') # libri-light too large, no silence trim\n",
    "    for audio_filepath in tqdm(sorted(list(audio_dir.glob(f'**/*.{ext}'))), desc=f'prepare..'):\n",
    "        audio_id = audio_filepath.stem\n",
    "        x, sr = torchaudio.load(audio_filepath)\n",
    "        length = x.shape[-1]\n",
    "        utt_len = length / sr\n",
    "        audio_len += utt_len\n",
    "        line = '{},{},{},{}\\n'.format(audio_id, audio_filepath, sr, length)\n",
    "        f.write(line)\n",
    "    print('Total audio len: {:.2f}h'.format(audio_len/3600))\n",
    "\n",
    "\n",
    "## get dataloader\n",
    "dataset = DatasetAudioTrain(csv_file=audio_manifest,\n",
    "                            sample_rate=cfg.sample_rate,\n",
    "                            n_examples=cfg.dataset.n_examples,\n",
    "                            chunk_size=cfg.dataset.segment_duration,\n",
    "                            trim_silence=cfg.dataset.trim_silence,\n",
    "                            normalize=cfg.dataset.normalize,\n",
    "                            lufs_norm_db=cfg.dataset.lufs_norm_db,\n",
    "                            lufs_var=cfg.dataset.lufs_var)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, \n",
    "                        batch_size=cfg.dataset.batch_size, num_workers=cfg.dataset.num_workers,\n",
    "                        shuffle=cfg.dataset.shuffle, drop_last=cfg.dataset.drop_last)\n",
    "print('Batch size: {}, {} iterations per epoch'.format(cfg.dataset.batch_size, len(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.solvers.builders import (\n",
    "    get_optimizer,\n",
    "    get_audio_datasets,\n",
    "    get_adversarial_losses,\n",
    "    get_loss,\n",
    "    get_balancer\n",
    ")\n",
    "from audiocraft.models.builders import get_compression_model\n",
    "\n",
    "\n",
    "## get model and optimizer\n",
    "model = get_compression_model(cfg.model)\n",
    "optimizer = get_optimizer(model.parameters(), cfg.optim)\n",
    "print('Use {} optimizer, learning rate: {}'.format(cfg.optim.optimizer, cfg.optim.lr))\n",
    "\n",
    "# get loss function\n",
    "adv_losses = get_adversarial_losses(cfg)\n",
    "aux_losses = torch.nn.ModuleDict()\n",
    "info_losses = torch.nn.ModuleDict()\n",
    "loss_weights = dict()\n",
    "for loss_name, weight in cfg.losses.items():\n",
    "    if loss_name in ['adv', 'feat']:\n",
    "        for adv_name, _ in adv_losses.items():\n",
    "            loss_weights[f'{loss_name}_{adv_name}'] = weight\n",
    "    elif weight > 0:\n",
    "        aux_losses[loss_name] = get_loss(loss_name, cfg)\n",
    "        loss_weights[loss_name] = weight\n",
    "    else:\n",
    "        info_losses[loss_name] = get_loss(loss_name, cfg)\n",
    "balancer = get_balancer(loss_weights, cfg.balancer)\n",
    "print(\"Total # of params: {:.2f} M\".format(sum(p.numel() for p in model.parameters())/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "ckpt_path = 'last_ckpt.pth'\n",
    "total_epoch = cfg.optim.epochs\n",
    "model = model.to(cfg.device)\n",
    "model.train()\n",
    "print('Training epoch: {}'.format(total_epoch))\n",
    "for epo in range(total_epoch):\n",
    "    for audio_data in tqdm(dataloader, total=len(dataloader)):\n",
    "        # prepare data\n",
    "        x = audio_data.to(cfg.device)\n",
    "        y = x.clone()\n",
    "        metrics = {}\n",
    "\n",
    "        # forward\n",
    "        qres = model(x)\n",
    "        y_pred = qres.x\n",
    "\n",
    "        # discrimilator loss\n",
    "        d_losses: dict = {}\n",
    "        for adv_name, adversary in adv_losses.items():\n",
    "            disc_loss = adversary.train_adv(y_pred, y)\n",
    "            d_losses[f'd_{adv_name}'] = disc_loss\n",
    "        metrics['d_loss'] = torch.sum(torch.stack(list(d_losses.values())))\n",
    "        \n",
    "        balanced_losses: dict = {}\n",
    "        other_losses: dict = {}\n",
    "\n",
    "        # penalty from quantization\n",
    "        if qres.penalty is not None and qres.penalty.requires_grad:\n",
    "            other_losses['penalty'] = qres.penalty  # penalty term from the quantizer\n",
    "\n",
    "        # adversarial losses\n",
    "        for adv_name, adversary in adv_losses.items():\n",
    "            adv_loss, feat_loss = adversary(y_pred, y)\n",
    "            balanced_losses[f'adv_{adv_name}'] = adv_loss\n",
    "            balanced_losses[f'feat_{adv_name}'] = feat_loss\n",
    "\n",
    "        # auxiliary losses\n",
    "        for loss_name, criterion in aux_losses.items():\n",
    "            loss = criterion(y_pred, y)\n",
    "            balanced_losses[loss_name] = loss\n",
    "\n",
    "        # backprop losses that are not handled by balancer\n",
    "        other_loss = torch.tensor(0., device=cfg.device)\n",
    "        if 'penalty' in other_losses:\n",
    "            other_loss += other_losses['penalty']\n",
    "        if other_loss.requires_grad:\n",
    "            other_loss.backward(retain_graph=True)\n",
    "        \n",
    "        # balancer losses backward\n",
    "        metrics['g_loss'] = balancer.backward(balanced_losses, y_pred)\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # save model every epoch\n",
    "    print('====> Epoch: {}, d_loss: {:.3f}, g_loss: {:.3f}'.format(epo, metrics['d_loss'], metrics['g_loss']))\n",
    "    torch.save({\n",
    "            'epoch': epo,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': metrics,\n",
    "            }, ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "ckpt_path = 'last_ckpt.pth'\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.cpu().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Compression\n",
    "Now we use the pretrained EnCodec model from [facebook/encodec_32khz](https://huggingface.co/facebook/encodec_32khz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load an audio, re-sample to 32kHz\n",
    "# x, fs = torchaudio.load('example.wav')\n",
    "x, fs = torchaudio.load('/home/xbie/Data/toy_dataset/LJ001-0002.wav')\n",
    "x = julius.resample_frac(x, old_sr=fs, new_sr=32000)\n",
    "print('Audio length: {:.1f}s'.format(x.shape[-1]/32000))\n",
    "\n",
    "## Encoder\n",
    "codes, scale = model.encode(x[None,])\n",
    "y = model.decode(codes, scale)\n",
    "torchaudio.save('example_recon.wav', y[0], sample_rate=32000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "$z = Enc(x)$ \n",
    "\n",
    "$z_q = Quant(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "codes, scale = model.encode(x[None,])\n",
    "\n",
    "print('Code dimension: {}'.format(codes.shape))\n",
    "\n",
    "codebook_size = model.quantizer.bins\n",
    "total_bits = codes.shape[-1] * model.num_codebooks * np.log2(codebook_size)\n",
    "audio_len = x.shape[-1] / fs\n",
    "bps = total_bits / audio_len\n",
    "print('Bandwidth: {:.1f} kbps'.format(bps/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "$\\hat{z} = Dequant(z_q)$\n",
    "\n",
    "$y = Dec(\\hat{z})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "y = model.decode(codes, scale)\n",
    "torchaudio.save('example_recon.wav', y[0], sample_rate=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import CompressionModel\n",
    "## Use the pretrained model from MetaAI\n",
    "model_fb = CompressionModel.get_pretrained('facebook/encodec_32khz')\n",
    "codes, scale = model_fb.encode(x[None,])\n",
    "y_fb = model_fb.decode(codes, scale)\n",
    "torchaudio.save('example_recon_fb.wav', y[0], sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the original audio and the reconstructed audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Audio:')\n",
    "IPython.display.Audio('example.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reconstructed Audio from our training')\n",
    "IPython.display.Audio('example_recon.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reconstructed Audio from our training')\n",
    "IPython.display.Audio('example_recon.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Audio Generation\n",
    "\n",
    "Now we use the pretrained AudioGen model from [facebook/audiogen-medium](https://huggingface.co/facebook/audiogen-medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import AudioGen\n",
    "from audiocraft.data.audio import audio_write\n",
    "\n",
    "\n",
    "## you can use any customized text prompt\n",
    "description = 'dog barking'\n",
    "# description = 'sirene of an emergency vehicle'\n",
    "# description = 'footsteps in a corridor'\n",
    "\n",
    "model = AudioGen.get_pretrained('facebook/audiogen-medium')\n",
    "model.set_generation_params(duration=5)\n",
    "wav = model.generate([description], progress=True)[0]\n",
    "audio_write('sample', wav.cpu(), model.sample_rate, strategy=\"loudness\", loudness_compressor=True) # -14 db LUFS\n",
    "\n",
    "print('Description {}'.format(description))\n",
    "IPython.display.Audio('sample.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiocraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
